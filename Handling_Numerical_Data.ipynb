{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Handling Numerical Data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNdnLOzj4K/FEJK9TLVghrh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bluelord/ML_Cookbook/blob/main/Handling_Numerical_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ2yG8loALHJ"
      },
      "source": [
        "## **Handling Numerical Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo6wuAem_r6L"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Datafiles/Boston-housing.csv')\n",
        "#dataframe = data.drop(data.iloc[:,0], axis=1)\n",
        "\n",
        "features = data.iloc[:,1:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIVsstfeA-uy"
      },
      "source": [
        "# Rescaling a Features \n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "features = data.iloc[:,1:10] # Using 10 features from the dataset\n",
        "\n",
        "# MinMaxScaler\n",
        "minmax_scale = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_features = minmax_scale.fit_transform(features)\n",
        "print(\"MinMaxScaled: \\n{}\\n\" .format(scaled_features))\n",
        "\n",
        "# StandardScaler\n",
        "scaler = StandardScaler()\n",
        "standardized = scaler.fit_transform(features)\n",
        "print(\"Standardized: \\n {}\\n:\" .format(standardized))\n",
        "\n",
        "# If our data has significant outliers, it can negatively impact our standardization by\n",
        "# affecting the feature’s mean and variance. In this scenario, it is often helpful to instead\n",
        "# rescale the feature using the median and quartile range.\n",
        "\n",
        "# RobustScaler\n",
        "robust_scaler = RobustScaler()\n",
        "scaled = robust_scaler.fit_transform(features)\n",
        "print(\"Standardized: \\n {}\\n:\" .format(scaled))\n",
        "\n",
        "# Normalization L2 & L2 \n",
        "L2_normalizer = Normalizer(norm='l2')\n",
        "L2_normalized = L2_normalizer.transform(features)\n",
        "print(\"L2_Normalized: \\n {}\\n:\" .format(L2_normalized))\n",
        "\n",
        "L1_normalizer = Normalizer(norm='l1')\n",
        "L1_normalized = L1_normalizer.transform(features)\n",
        "print(\"L1_Normalized: \\n {}\\n:\" .format(L1_normalized))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1QwNQTvA_PJ"
      },
      "source": [
        "# Generating Polynomial & interaction features\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "polynomial_interaction = PolynomialFeatures(degree= 2, include_bias=False)\n",
        "polynoamial_features = polynomial_interaction.fit_transform(features)\n",
        "print(\"Polynomial features: \\n {}\\n:\" .format(polynoamial_features))\n",
        "\n",
        "# We can restrict the features created to only interaction features by setting interaction_only to True:\n",
        "interaction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "new_poly_features = interaction.fit_transform(features)\n",
        "print(\"Polynomial features: \\n {}\\n:\" .format(new_poly_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMYY69a_P8qn"
      },
      "source": [
        "# Transforming Features \n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "def add_ten(x):\n",
        "  return x+10\n",
        "\n",
        "transformer = FunctionTransformer(add_ten)\n",
        "transformed = transformer.transform(features)\n",
        "\n",
        "print(\"Transformed Features: \\n{}\\n\" .format(transformed))\n",
        "\n",
        "# Same transformation can be done on pandas too using apply()\n",
        "apply = features.apply(add_ten)\n",
        "\n",
        "print(\"Tranformation using apply function: \\n{}\\n\" .format(apply))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr64ETpvRuwx"
      },
      "source": [
        "# Deleting the outliers \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.datasets import make_blobs\n",
        "feature, _ = make_blobs(n_samples = 10,\n",
        "                         n_features = 2,\n",
        "                         centers = 1,\n",
        "                         random_state = 1)\n",
        "# replacing the obser vation values with the extreme values\n",
        "feature[0,0] = 10000\n",
        "feature[0,1] = 10000\n",
        "\n",
        "outliers_detection = EllipticEnvelope(contamination = 0.1)\n",
        "outliers_detection.fit(feature)\n",
        "outliers_detection.predict(feature)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUgeY1o4kZot"
      },
      "source": [
        "A major limitation of this approach is the need to specify a contamination parameter, which is the proportion of observations that are outliers—a value that we don’t know.\n",
        "\n",
        "Instead of looking at observations as a whole, we can instead look at individual features and identify extreme values in those features using interquartile range (IQR):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYUyFkr7Usbi"
      },
      "source": [
        "feature = features.iloc[:,0]\n",
        "\n",
        "# Create a function to return index of outliers\n",
        "def indicies_of_outliers(x):\n",
        "  q1, q3 = np.percentile(x, [25, 75])\n",
        "  iqr = q3 - q1\n",
        "  lower_bound = q1 - (iqr * 1.5)\n",
        "  upper_bound = q3 + (iqr * 1.5)\n",
        "  return np.where((x > upper_bound) | (x < lower_bound))\n",
        "\n",
        "indicies_of_outliers(feature)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coPrz0sgt0wD"
      },
      "source": [
        "Handling the outliers:\n",
        "\n",
        "*   Typically we have three strategies we can use to handle outliers. First, we can drop.\n",
        "*   We can mark them as outliers and include it as a feature.\n",
        "*   We can transform the feature to dampen the effect of the outlier.\n",
        "\n",
        "We can handle them based on two aspects.\n",
        "*   we should consider what makes them an outlier.\n",
        "\n",
        "If we believe they are errors in the data such as from a broken sensor or a miscoded value, then we might drop the observation or replace outlier\n",
        "values with NaN.\n",
        "\n",
        "If we believe the outliers are genuine extreme values, then marking them as outliers or transforming their values is more appropriate.\n",
        "\n",
        "\n",
        "*   How we handle outliers should be based on our goal for machine learning.\n",
        "\n",
        "If we want to predict house prices based on features of the house, we\n",
        "might reasonably assume the price for mansions with over 100 bathrooms is driven\n",
        "by a different dynamic than regular family homes.\n",
        "\n",
        "If we are training a model to use as part of an online home loan web application, we might assume that nour potential users will not include billionaires looking to buy a mansion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3We9TtCCpb19"
      },
      "source": [
        "# Handlng Outliers\n",
        "houses = pd.DataFrame()\n",
        "houses['Price'] = [534433, 392333, 293222, 4322032]\n",
        "houses['Bathrooms'] = [2, 3.5, 2, 116]\n",
        "houses['Square_Feet'] = [1500, 2500, 1500, 48000]\n",
        "\n",
        "#  (1) Filter observations\n",
        "print(\"Filter value: \\n{}\\n\" \n",
        "      .format(houses[houses['Bathrooms'] < 20]))\n",
        "# (2) Features based on boolean condition\n",
        "print(\"Features based on boolean condition: \\n{}\\n\" \n",
        "      .format(np.where(houses[\"Bathrooms\"] < 20, 0, 1)))\n",
        "\n",
        "# (3) Log feature\n",
        "print(\"Transfoemed features: \\n{}\\n\" \n",
        "      .format([np.log(x) for x in houses[\"Square_Feet\"]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiaiY_oXuKwL"
      },
      "source": [
        " # Discretizating Features\n",
        "\n",
        " from sklearn.preprocessing import Binarizer \n",
        "\n",
        " age = np.array([[6],\n",
        "                [8],\n",
        "                [15],\n",
        "                [30],\n",
        "                [50]])\n",
        "# Breaking up the features by Binerizing the data by thresholding \n",
        "binerizer = Binarizer(16)\n",
        "print(\"Binerized features:\\n{}\\n\" \n",
        "      .format(binerizer.fit_transform(age)))\n",
        "\n",
        "# Breaking the features to multiple thresholds\n",
        "print(\"Breaking wiht multiple threshold: \\n{}\\n\" \n",
        "      .format(np.digitize(age, bins=[10,20,40])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqoTDxBQHZwt"
      },
      "source": [
        "# Grouping Observations Using Clustering\n",
        "# we can use clustering as a preprocessing step.\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "features, _ = make_blobs(n_samples = 50,\n",
        "                         n_features = 2,\n",
        "                         centers = 3,\n",
        "                         random_state = 1)\n",
        "\n",
        "\n",
        "dataframe = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n",
        "\n",
        "# Make k-means clusterer\n",
        "clusterer = KMeans(3, random_state=0)\n",
        "clusterer.fit(features)\n",
        "\n",
        "# Predict values\n",
        "dataframe[\"group\"] = clusterer.predict(features)\n",
        "# View first few observations\n",
        "dataframe.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrlkcJItHajY"
      },
      "source": [
        "# Deleting Observations with Missing Values\n",
        "\n",
        "features = np.array([[1.1, 11.1],\n",
        "                     [2.2, 22.2],\n",
        "                     [3.3, 33.3],\n",
        "                     [4.4, 44.4],\n",
        "                     [np.nan, 55]])\n",
        "\n",
        "# Keep only observations that are not (denoted by ~) missing\n",
        "print(\"Values which are not NaN: \\n {}\\n: \" \n",
        "      .format(features[~np.isnan(features).any(axis=1)]))\n",
        "\n",
        "# Droping the missing values \n",
        "dataframe = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n",
        "print(\"Removed observations with missing values: \\n {}\\n\" \n",
        "      .format(dataframe.dropna()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyZsPVU4ZowI"
      },
      "source": [
        "Depending on the cause of the missing values, deleting observations can introduce bias into our data. There are three types of missing data:\n",
        "*   Missing Completely At Random (MCAR): The probability that a value is missing is independent of everything.\n",
        "*   Missing At Random (MAR): The probability that a value is missing is not completely random, but depends on the information captured in other features.\n",
        "*   Missing Not At Random (MNAR): The probability that a value is missing is not random and depends on information not captured in our features.\n",
        "\n",
        "It is sometimes acceptable to delete observations if they are MCAR or MAR, if the value is MNAR, the fact that a value is missing is itself information. Deleting MNAR observations can inject bias into our data because we are removing observations produced by some unobserved systematic effect.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJsq32u1PHG0"
      },
      "source": [
        "# Imputing Missing Values\n",
        "# If you have missing values in your data and want to fill in or predict their values.\n",
        "\n",
        "from sklearn.Imputer import \n",
        "\n",
        "mean_imputer = Imputer(strategy=\"mean\", axis=0)\n",
        "\n",
        "features_mean_imputed = mean_imputer.fit_transform(features)\n",
        "# Compare true and imputed values\n",
        "print(\"True Value:\", true_value)\n",
        "print(\"Imputed Value:\", features_mean_imputed[0,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4DLfXHucaGW"
      },
      "source": [
        "There are two main strategies for replacing missing data with substitute values, each of which has strengths and weaknesses.\n",
        "\n",
        "\n",
        "First, we can use machine learning to predict the values of the missing data, for this we treat the feature with missing values as a target vector and use the remaining subset of features to predict missing values.\n",
        "\n",
        "An alternative and more scalable strategy is to fill in all missing values with some average value.\n"
      ]
    }
  ]
}