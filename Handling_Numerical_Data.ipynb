{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Handling Numerical Data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOK45Rj+rkVvkCcZWh0SjEc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bluelord/ML_Cookbook/blob/main/Handling_Numerical_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ2yG8loALHJ"
      },
      "source": [
        "## **Handling Numerical Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JQtD1P2RiC8",
        "outputId": "c4376eff-46ea-4949-90b7-d2ceb68e6e16"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo6wuAem_r6L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "7364788f-8f43-4548-8bbb-9b18976cebfe"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/ML Cookbook/Datasets/Boston-housing.csv\", index_col=0)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data.iloc[:,-1])\n",
        "data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>crim</th>\n",
              "      <th>zn</th>\n",
              "      <th>indus</th>\n",
              "      <th>chas</th>\n",
              "      <th>nox</th>\n",
              "      <th>rm</th>\n",
              "      <th>age</th>\n",
              "      <th>dis</th>\n",
              "      <th>rad</th>\n",
              "      <th>tax</th>\n",
              "      <th>ptratio</th>\n",
              "      <th>black</th>\n",
              "      <th>lstat</th>\n",
              "      <th>medv</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1</td>\n",
              "      <td>296</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      crim    zn  indus  chas    nox  ...  tax  ptratio   black  lstat  medv\n",
              "1  0.00632  18.0   2.31     0  0.538  ...  296     15.3  396.90   4.98  24.0\n",
              "2  0.02731   0.0   7.07     0  0.469  ...  242     17.8  396.90   9.14  21.6\n",
              "3  0.02729   0.0   7.07     0  0.469  ...  242     17.8  392.83   4.03  34.7\n",
              "4  0.03237   0.0   2.18     0  0.458  ...  222     18.7  394.63   2.94  33.4\n",
              "5  0.06905   0.0   2.18     0  0.458  ...  222     18.7  396.90   5.33  36.2\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIVsstfeA-uy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a175ed3-7721-43cb-d226-0098021b07df"
      },
      "source": [
        "# Rescaling a Features \n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# MinMaxScaler\n",
        "minmax_scale = MinMaxScaler(feature_range=(0,1))\n",
        "scaled_features = minmax_scale.fit_transform(X_train)\n",
        "print(\"MinMaxScaled: \\n{}\\n\" .format(scaled_features))\n",
        "\n",
        "# StandardScaler\n",
        "scaler = StandardScaler()\n",
        "standardized = scaler.fit_transform(X_train)\n",
        "print(\"Standardized: \\n {}\\n:\" .format(standardized))\n",
        "\n",
        "# RobustScaler\n",
        "robust_scaler = RobustScaler()\n",
        "scaled = robust_scaler.fit_transform(X_train)\n",
        "print(\"Standardized: \\n {}\\n:\" .format(scaled))\n",
        "\n",
        "# Normalization L2 & L2 \n",
        "L2_normalizer = Normalizer(norm='l2')\n",
        "L2_normalized = L2_normalizer.transform(X_train)\n",
        "print(\"L2_Normalized: \\n {}\\n:\" .format(L2_normalized))\n",
        "\n",
        "L1_normalizer = Normalizer(norm='l1')\n",
        "L1_normalized = L1_normalizer.transform(X_train)\n",
        "print(\"L1_Normalized: \\n {}\\n:\" .format(L1_normalized))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MinMaxScaled: \n",
            "[[3.87569366e-03 0.00000000e+00 2.53665689e-01 ... 7.44680851e-01\n",
            "  1.00000000e+00 1.69361702e-01]\n",
            " [1.26389965e-02 0.00000000e+00 2.81524927e-01 ... 8.93617021e-01\n",
            "  9.07383126e-01 5.92056738e-01]\n",
            " [3.31797683e-04 8.42105263e-01 1.06671554e-01 ... 3.72340426e-01\n",
            "  1.00000000e+00 8.34042553e-02]\n",
            " ...\n",
            " [1.50837564e-03 0.00000000e+00 3.71334311e-01 ... 6.38297872e-01\n",
            "  9.72035907e-01 2.17021277e-01]\n",
            " [7.63342381e-01 0.00000000e+00 6.46627566e-01 ... 8.08510638e-01\n",
            "  9.69917797e-01 6.02836879e-01]\n",
            " [3.14151261e-04 0.00000000e+00 1.73387097e-01 ... 8.08510638e-01\n",
            "  1.00000000e+00 2.28936170e-01]]\n",
            "\n",
            "Standardized: \n",
            " [[-0.3870016  -0.48047944 -0.57784798 ...  0.508366    0.44298387\n",
            "  -0.72236702]\n",
            " [-0.29863303 -0.48047944 -0.46733151 ...  1.15662122  0.04530852\n",
            "   1.34405319]\n",
            " [-0.42273801  3.09671799 -1.16096779 ... -1.11227206  0.44298387\n",
            "  -1.14258536]\n",
            " ...\n",
            " [-0.41087348 -0.48047944 -0.1110613  ...  0.04532655  0.32291255\n",
            "  -0.48937468]\n",
            " [ 7.27141052 -0.48047944  0.98101594 ...  0.78618966  0.31381788\n",
            "   1.39675384]\n",
            " [-0.42291596 -0.48047944 -0.89630992 ...  0.78618966  0.44298387\n",
            "  -0.43112659]]\n",
            ":\n",
            "Standardized: \n",
            " [[ 1.67313736e-02  0.00000000e+00 -1.83042789e-01 ...  1.78571429e-01\n",
            "   2.73276904e-01 -3.98203593e-01]\n",
            " [ 2.28706513e-01  0.00000000e+00 -1.22820919e-01 ...  6.78571429e-01\n",
            "  -1.50326481e+00  1.08882236e+00]\n",
            " [-6.89917675e-02  6.40000000e+00 -5.00792393e-01 ... -1.07142857e+00\n",
            "   2.73276904e-01 -7.00598802e-01]\n",
            " ...\n",
            " [-4.05315759e-02  0.00000000e+00  7.13153724e-02 ... -1.78571429e-01\n",
            "  -2.63119710e-01 -2.30538922e-01]\n",
            " [ 1.83874371e+01  0.00000000e+00  6.66402536e-01 ...  3.92857143e-01\n",
            "  -3.03748489e-01  1.12674651e+00]\n",
            " [-6.94186160e-02  0.00000000e+00 -3.56576862e-01 ...  3.92857143e-01\n",
            "   2.73276904e-01 -1.88622754e-01]]\n",
            ":\n",
            "L2_Normalized: \n",
            " [[7.12366302e-04 0.00000000e+00 1.49719864e-02 ... 3.97629992e-02\n",
            "  8.05200733e-01 1.56211782e-02]\n",
            " [2.33809974e-03 0.00000000e+00 1.68305302e-02 ... 4.34202868e-02\n",
            "  7.44699271e-01 4.67284991e-02]\n",
            " [6.79491096e-05 1.51672120e-01 6.38918804e-03 ... 3.05240141e-02\n",
            "  7.52483304e-01 8.85385999e-03]\n",
            " ...\n",
            " [2.94778331e-04 0.00000000e+00 2.22153610e-02 ... 3.90184810e-02\n",
            "  8.09339794e-01 1.96770619e-02]\n",
            " [8.70889948e-02 0.00000000e+00 2.32080718e-02 ... 2.59007211e-02\n",
            "  4.93613890e-01 2.94652757e-02]\n",
            " [7.47001880e-05 0.00000000e+00 1.13129260e-02 ... 4.40310416e-02\n",
            "  8.65144575e-01 2.13615945e-02]]\n",
            ":\n",
            "L1_Normalized: \n",
            " [[4.47262966e-04 0.00000000e+00 9.40024118e-03 ... 2.49654102e-02\n",
            "  5.05549556e-01 9.80783971e-03]\n",
            " [1.36468277e-03 0.00000000e+00 9.82350504e-03 ... 2.53431948e-02\n",
            "  4.34659928e-01 2.72741049e-02]\n",
            " [4.10455934e-05 9.16196282e-02 3.85947684e-03 ... 1.84384502e-02\n",
            "  4.54547880e-01 5.34829580e-03]\n",
            " ...\n",
            " [1.87702891e-04 0.00000000e+00 1.41458412e-02 ... 2.48453869e-02\n",
            "  5.15354770e-01 1.25295553e-02]\n",
            " [5.17699994e-02 0.00000000e+00 1.37960240e-02 ... 1.53966677e-02\n",
            "  2.93428474e-01 1.75156150e-02]\n",
            " [4.76608117e-05 0.00000000e+00 7.21796360e-03 ... 2.80930375e-02\n",
            "  5.51986465e-01 1.36292954e-02]]\n",
            ":\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nw6ilJZU5s1"
      },
      "source": [
        "**min-max scaling**\n",
        "\n",
        "Rescaling is a common preprocessing task in machine learning, many of the algorithms assume all features are on the same scale, typically\n",
        "0 to 1 or –1 to 1.\n",
        "\n",
        "There are a number of rescaling techniques, but one of the\n",
        "simplest is called min-max scaling, min-max calculates:\n",
        "\n",
        "$x_i^′ = \\frac{x_i − min(x)}{max(x) − min(x)}$\n",
        "\n",
        "where $x$ is the feature vector, $x_i$ is an individual element of feature $x$, and $x_i^`$ is rescaled element."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLxHcMCFXaLn"
      },
      "source": [
        "**Standerdizing Features**\n",
        "\n",
        "To achieve standard normally distributed, we use standardization to transform the data such that it has a mean, $\\bar{x}$, of $0$ and a standard deviation, $\\sigma$, of $1$. This standardized value is also called a z-score in statistics.\n",
        "\n",
        "$x_i^′ = \\frac{x_i − \\bar{x}} {\\sigma}$\n",
        "\n",
        "Standardization is a common go-to scaling method for machine learning preprocessing. However, it depends on the learning algorithm. For example, PCA often works better using standardization, while min-max scaling is often recommended for neural networks.\n",
        "\n",
        "If our data has significant outliers, it can negatively impact our standardization by affecting the feature’s mean and variance. In this scenario, it is often helpful to instead rescale the feature using the median and quartile range, by using the RobustScaler method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8vIJcRKZ66E"
      },
      "source": [
        "**Normalizing Obsevations**\n",
        "\n",
        "We can also rescale across individual observations. Normalizer\n",
        "rescales the values on individual observations to have unit norm (the sum of their lengths is 1). This type of rescaling is often used when we have many equivalent features.\n",
        "\n",
        "Euclidean norm (often called **L2**) being the default argument:$||x||_2 = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}$\n",
        "\n",
        "Manhattan norm (**L1**): $||x||_1 = \\sum_{i=1}^{n}|x_i|$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1QwNQTvA_PJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "954ba2d2-fd1c-45f3-88d6-536862d3741b"
      },
      "source": [
        "# Generating Polynomial & interaction features\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "polynomial_interaction = PolynomialFeatures(degree= 2, include_bias=False)\n",
        "polynoamial_features = polynomial_interaction.fit_transform(X_train)\n",
        "print(\"Polynomial features: \\n {}\\n:\" .format(polynoamial_features))\n",
        "\n",
        "# We can restrict the features created to only interaction features by setting interaction_only to True:\n",
        "interaction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "new_poly_features = interaction.fit_transform(X_train)\n",
        "print(\"Polynomial features: \\n {}\\n:\" .format(new_poly_features))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Polynomial features: \n",
            " [[3.51140000e-01 0.00000000e+00 7.38000000e+00 ... 1.57529610e+05\n",
            "  3.05613000e+03 5.92900000e+01]\n",
            " [1.13081000e+00 0.00000000e+00 8.14000000e+00 ... 1.29722429e+05\n",
            "  8.13984200e+03 5.10760000e+02]\n",
            " [3.58400000e-02 8.00000000e+01 3.37000000e+00 ... 1.57529610e+05\n",
            "  1.85352300e+03 2.18089000e+01]\n",
            " ...\n",
            " [1.40520000e-01 0.00000000e+00 1.05900000e+01 ... 1.48849356e+05\n",
            "  3.61889780e+03 8.79844000e+01]\n",
            " [6.79208000e+01 0.00000000e+00 1.81000000e+01 ... 1.48201901e+05\n",
            "  8.84661060e+03 5.28080400e+02]\n",
            " [3.42700000e-02 0.00000000e+00 5.19000000e+00 ... 1.57529610e+05\n",
            "  3.88962000e+03 9.60400000e+01]]\n",
            ":\n",
            "Polynomial features: \n",
            " [[3.5114000e-01 0.0000000e+00 7.3800000e+00 ... 7.7792400e+03\n",
            "  1.5092000e+02 3.0561300e+03]\n",
            " [1.1308100e+00 0.0000000e+00 8.1400000e+00 ... 7.5635700e+03\n",
            "  4.7460000e+02 8.1398420e+03]\n",
            " [3.5840000e-02 8.0000000e+01 3.3700000e+00 ... 6.3900900e+03\n",
            "  7.5187000e+01 1.8535230e+03]\n",
            " ...\n",
            " [1.4052000e-01 0.0000000e+00 1.0590000e+01 ... 7.1760660e+03\n",
            "  1.7446800e+02 3.6188978e+03]\n",
            " [6.7920800e+01 0.0000000e+00 1.8100000e+01 ... 7.7763940e+03\n",
            "  4.6419600e+02 8.8466106e+03]\n",
            " [3.4270000e-02 0.0000000e+00 5.1900000e+00 ... 8.0173800e+03\n",
            "  1.9796000e+02 3.8896200e+03]]\n",
            ":\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb1LDoGkdbVO"
      },
      "source": [
        "Polynomial features are often created when we want to include the notion that there exists a nonlinear relationship between the features and the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMYY69a_P8qn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec7ed48-3a27-4faf-a209-97499e6a1ebc"
      },
      "source": [
        "# Transforming Features \n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "def add_ten(x):\n",
        "  return x+10\n",
        "\n",
        "trans = FunctionTransformer(add_ten)\n",
        "transformed = trans.transform(X_train.iloc[:,2:4])\n",
        "print(\"Transformed Features: \\n{}\\n\" .format(transformed))\n",
        "\n",
        "# Same transformation can be done on pandas too using apply()\n",
        "print(\"Tranformation using apply function: \\n{}\\n\" .format(X_train.iloc[:,2:4].apply(add_ten)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transformed Features: \n",
            "     indus  chas\n",
            "323  17.38    10\n",
            "31   18.14    10\n",
            "66   13.37    10\n",
            "102  18.56    10\n",
            "135  31.89    10\n",
            "..     ...   ...\n",
            "215  20.59    10\n",
            "291  14.95    10\n",
            "214  20.59    10\n",
            "406  28.10    10\n",
            "337  15.19    10\n",
            "\n",
            "[379 rows x 2 columns]\n",
            "\n",
            "Tranformation using apply function: \n",
            "     indus  chas\n",
            "323  17.38    10\n",
            "31   18.14    10\n",
            "66   13.37    10\n",
            "102  18.56    10\n",
            "135  31.89    10\n",
            "..     ...   ...\n",
            "215  20.59    10\n",
            "291  14.95    10\n",
            "214  20.59    10\n",
            "406  28.10    10\n",
            "337  15.19    10\n",
            "\n",
            "[379 rows x 2 columns]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOvE8Nv0etrx"
      },
      "source": [
        "**Deleting the outliers**\n",
        "A common method is to assume the data is normally distributed and based on that assumption “draw” an ellipse around the data, classifying any observation inside the ellipse as an inlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWeIH-7vetoo"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr64ETpvRuwx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fca61e7-68ec-4a46-a677-954fba8c2b02"
      },
      "source": [
        "# Deleting the outliers \n",
        "\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.datasets import make_blobs\n",
        "feature, _ = make_blobs(n_samples = 10, n_features = 2, centers = 1, random_state = 70)\n",
        "\n",
        "# replacing the obser vation values with the extreme values\n",
        "feature[0,0] = 10000\n",
        "feature[0,1] = 10000\n",
        "\n",
        "outliers_detection = EllipticEnvelope(contamination = 0.1)\n",
        "outliers_detection.fit(feature)\n",
        "outliers_detection.predict(feature)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1,  1,  1,  1,  1,  1,  1,  1,  1,  1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUgeY1o4kZot"
      },
      "source": [
        "A major limitation of this approach is the need to specify a contamination parameter, which is the proportion of observations that are outliers—a value that we don’t know. Instead of looking at observations as a whole, we can instead look at individual features and identify extreme values in those features using interquartile range (IQR):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYUyFkr7Usbi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cd5c51c-1717-4ad0-f006-1ccb7f20edd6"
      },
      "source": [
        "feature = X_train\n",
        "\n",
        "# Create a function to return index of outliers\n",
        "def indicies_of_outliers(x):\n",
        "  q1, q3 = np.percentile(x, [25, 75])\n",
        "  iqr = q3 - q1\n",
        "  lower_bound = q1 - (iqr * 1.5)\n",
        "  upper_bound = q3 + (iqr * 1.5)\n",
        "  return np.where((x > upper_bound) | (x < lower_bound))\n",
        "\n",
        "indicies_of_outliers(feature)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  0,   0,   1, ..., 377, 378, 378]),\n",
              " array([ 9, 11,  6, ..., 11,  9, 11]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coPrz0sgt0wD"
      },
      "source": [
        "Handling the outliers:\n",
        "\n",
        "*   Typically we have three strategies we can use to handle outliers. First, we can drop.\n",
        "*   We can mark them as outliers and include it as a feature.\n",
        "*   We can transform the feature to dampen the effect of the outlier.\n",
        "\n",
        "We can handle them based on two aspects.\n",
        "> we should consider what makes them an outlier.\n",
        "If we believe they are errors in the data such as from a broken sensor or a miscoded value, then we might drop the observation or replace outlier values with NaN. If we believe the outliers are genuine extreme values, then marking them as outliers or transforming their values is more appropriate.\n",
        "\n",
        "> How we handle outliers should be based on our goal for machine learning.\n",
        "If we want to predict house prices based on features of the house, we\n",
        "might reasonably assume the price for mansions with over 100 bathrooms is driven\n",
        "by a different dynamic than regular family homes.\n",
        "If we are training a model to use as part of an online home loan web application, we might assume that nour potential users will not include billionaires looking to buy a mansion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3We9TtCCpb19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb524d0a-08d0-472e-af4d-c2f4cf87d117"
      },
      "source": [
        "# Handlng Outliers\n",
        "houses = pd.DataFrame()\n",
        "houses['Price'] = [534433, 392333, 293222, 4322032]\n",
        "houses['Bathrooms'] = [2, 3.5, 2, 116]\n",
        "houses['Square_Feet'] = [1500, 2500, 1500, 48000]\n",
        "\n",
        "#  (1) Filter observations\n",
        "print(\"Filter value: \\n{}\\n\" \n",
        "      .format(houses[houses['Bathrooms'] < 20]))\n",
        "# (2) Features based on boolean condition\n",
        "print(\"Features based on boolean condition: \\n{}\\n\" \n",
        "      .format(np.where(houses[\"Bathrooms\"] < 20, 0, 1)))\n",
        "\n",
        "# (3) Log feature\n",
        "print(\"Transfoemed features: \\n{}\\n\" \n",
        "      .format([np.log(x) for x in houses[\"Square_Feet\"]]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filter value: \n",
            "    Price  Bathrooms  Square_Feet\n",
            "0  534433        2.0         1500\n",
            "1  392333        3.5         2500\n",
            "2  293222        2.0         1500\n",
            "\n",
            "Features based on boolean condition: \n",
            "[0 0 0 1]\n",
            "\n",
            "Transfoemed features: \n",
            "[7.313220387090301, 7.824046010856292, 7.313220387090301, 10.778956289890028]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH2aTdOhmH_H"
      },
      "source": [
        "**Discretizating Features**\n",
        "\n",
        "Depending on how we want to break up the data, there are two techniques\n",
        "*   We can binarize the feature according to some threshold\n",
        "*   We can break up numerical features according to multiple thresholds.\n",
        "\n",
        "Discretization can be a fruitful strategy when we have reason to believe that a \n",
        "feature should behave more like a categorical feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiaiY_oXuKwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1e102b-f61f-495e-e980-92594f318287"
      },
      "source": [
        " # Discretizating Features\n",
        "\n",
        " from sklearn.preprocessing import Binarizer \n",
        "\n",
        " age = np.array([[6],\n",
        "                [8],\n",
        "                [15],\n",
        "                [30],\n",
        "                [50]])\n",
        "# Breaking up the features by Binerizing the data by thresholding \n",
        "binerizer = Binarizer(16)\n",
        "print(\"Binerized features:\\n{}\\n\" \n",
        "      .format(binerizer.fit_transform(age)))\n",
        "\n",
        "# Breaking the features to multiple thresholds\n",
        "print(\"Breaking wiht multiple threshold: \\n{}\\n\" \n",
        "      .format(np.digitize(age, bins=[10,20,40])))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Binerized features:\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]]\n",
            "\n",
            "Breaking wiht multiple threshold: \n",
            "[[0]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [3]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqoTDxBQHZwt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "296470de-7ad3-452a-c1b4-a450f346f6f7"
      },
      "source": [
        "# Grouping Observations Using Clustering\n",
        "\n",
        "# we can use clustering as a preprocessing step.\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "features, _ = make_blobs(n_samples = 50, n_features = 2, centers = 3, random_state = 70)\n",
        "dataframe = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n",
        "\n",
        "# Make k-means clusterer\n",
        "clusterer = KMeans(3, random_state=0)\n",
        "clusterer.fit(features)\n",
        "\n",
        "# Predict values\n",
        "dataframe[\"group\"] = clusterer.predict(features)\n",
        "# View first few observations\n",
        "dataframe.head(5)\n",
        "\n",
        "# Details of this will be in K-Mean clustering algorithms"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>group</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11.639411</td>\n",
              "      <td>6.210485</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.514475</td>\n",
              "      <td>7.033964</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-3.929970</td>\n",
              "      <td>-7.583885</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8.346673</td>\n",
              "      <td>8.549041</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-2.775634</td>\n",
              "      <td>-6.824632</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   feature_1  feature_2  group\n",
              "0  11.639411   6.210485      1\n",
              "1   5.514475   7.033964      1\n",
              "2  -3.929970  -7.583885      0\n",
              "3   8.346673   8.549041      1\n",
              "4  -2.775634  -6.824632      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrlkcJItHajY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "219fb279-485a-4b6e-c088-c260ba8d78cd"
      },
      "source": [
        "# Deleting Observations with Missing Values\n",
        "\n",
        "dataframe = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/ML Cookbook/Datasets/titanic.csv\")\n",
        "\n",
        "# Droping the missing values \n",
        "print(\"Removed observations with missing values: \\n {}\\n\" \n",
        "      .format(dataframe.dropna()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removed observations with missing values: \n",
            "      PassengerId  Survived  Pclass  ...     Fare        Cabin  Embarked\n",
            "1              2         1       1  ...  71.2833          C85         C\n",
            "3              4         1       1  ...  53.1000         C123         S\n",
            "6              7         0       1  ...  51.8625          E46         S\n",
            "10            11         1       3  ...  16.7000           G6         S\n",
            "11            12         1       1  ...  26.5500         C103         S\n",
            "..           ...       ...     ...  ...      ...          ...       ...\n",
            "871          872         1       1  ...  52.5542          D35         S\n",
            "872          873         0       1  ...   5.0000  B51 B53 B55         S\n",
            "879          880         1       1  ...  83.1583          C50         C\n",
            "887          888         1       1  ...  30.0000          B42         S\n",
            "889          890         1       1  ...  30.0000         C148         C\n",
            "\n",
            "[183 rows x 12 columns]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyZsPVU4ZowI"
      },
      "source": [
        "Out of 891 only 183 remains, means we have to handel this NAN, whithout droping.\n",
        "\n",
        "Depending on the cause of the missing values, deleting observations can introduce bias into our data. There are three types of missing data:\n",
        "*   Missing Completely At Random (MCAR): The probability that a value is missing is independent of everything.\n",
        "*   Missing At Random (MAR): The probability that a value is missing is not completely random, but depends on the information captured in other features.\n",
        "*   Missing Not At Random (MNAR): The probability that a value is missing is not random and depends on information not captured in our features.\n",
        "\n",
        "It is sometimes acceptable to delete observations if they are MCAR or MAR, if the value is MNAR, the fact that a value is missing is itself information. Deleting MNAR observations can inject bias into our data because we are removing observations produced by some effect.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-VQJyGvtfLS",
        "outputId": "0e119a58-2281-4252-95e1-d155e7286003"
      },
      "source": [
        "# Imputing Missing Values\n",
        "\n",
        "# Load libraries\n",
        "from fancyimpute import KNN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_blobs\n",
        "# Make a simulated feature matrix\n",
        "features, _ = make_blobs(n_samples = 1000, n_features = 2, random_state = 1)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "standardized_features = scaler.fit_transform(features)\n",
        "\n",
        "# Replace the first feature's first value with a missing value\n",
        "true_value = standardized_features[0,0]\n",
        "standardized_features[0,0] = np.nan\n",
        "\n",
        "# Predict the missing values in the feature matrix\n",
        "features_knn_imputed = KNN(k=5, verbose=0).fit_transform(features)\n",
        "\n",
        "# Compare true and imputed values\n",
        "print(\"True Value:\", true_value)\n",
        "print(\"Imputed Value:\", features_knn_imputed[0,0])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Value: 0.8730186113995938\n",
            "Imputed Value: -3.058372724614996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fancyimpute/solver.py:58: UserWarning: Input matrix is not missing any values\n",
            "  warnings.warn(\"Input matrix is not missing any values\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBlmE6jP7CNk",
        "outputId": "c536bd6d-4a2d-457b-e87b-0100c63207df"
      },
      "source": [
        "# Load library\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "# Create imputer\n",
        "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "\n",
        "# Impute values\n",
        "features_mean_imputed = imp_mean.fit_transform(features)\n",
        "# Compare true and imputed values\n",
        "print(\"True Value:\", true_value)\n",
        "print(\"Imputed Value:\", features_mean_imputed[0,0])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True Value: 0.8730186113995938\n",
            "Imputed Value: -3.058372724614996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4DLfXHucaGW"
      },
      "source": [
        "There are two main strategies for replacing missing data with substitute values, each of which has strengths and weaknesses.\n",
        "*   We can use machine learning to predict the values of the missing data, for this we treat the feature with missing values as a target vector and use the remaining subset of features to predict missing values.\n",
        "\n",
        "*  An alternative and more scalable strategy is to fill in all missing values with some average value.\n"
      ]
    }
  ]
}